# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nurDG6Hjdy3CXalFTnZzo7QXzMHHLwWH
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
from filters.pii_filter import contains_pii, detect_pii
from filters.hate_classifier import HateClassifier
from utils.logger import ConversationLogger

# Choose a small model suitable for local testing; replace with an open LLaMA or larger model if available.
MODEL_NAME = 'distilgpt2'

class SafeChatbot:
    def __init__(self, model_name=MODEL_NAME, device=None):
        self.device = device if device is not None else (0 if torch.cuda.is_available() else -1)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        if torch.cuda.is_available():
            self.model.to('cuda')
        self.generator = pipeline('text-generation', model=self.model, tokenizer=self.tokenizer, device=0 if torch.cuda.is_available() else -1)

        # Load hate classifier
        self.hate_clf = HateClassifier()
        self.logger = ConversationLogger('logs/conversations.jsonl')

    def generate_response(self, prompt: str, max_length=128, num_return_sequences=1) -> str:
        # Generate raw candidate
        outputs = self.generator(prompt, max_length=max_length, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=num_return_sequences)
        return outputs[0]['generated_text'][len(prompt):].strip()

    def filter_and_respond(self, user_input: str) -> dict:
        # 1) If user input contains PII, reject immediately (don't echo PII back)
        input_pii = detect_pii(user_input)
        if input_pii:
            blocked = True
            reason = 'PII in user input'
            safe_text = "I'm sorry — I can't process messages that contain personal contact details or other sensitive personal information."
            self.logger.log(user_input, safe_text, blocked, reasons=[reason])
            return {'safe': False, 'text': safe_text, 'blocked': True, 'reason': reason}

        # 2) Generate candidate response from LLM
        candidate = self.generate_response(user_input)

        # 3) Check candidate for PII
        if contains_pii(candidate):
            blocked = True
            reason = 'PII in model output'
            safe_text = "I can't share personal contact details or other personally identifying information."
            self.logger.log(user_input, candidate, blocked, reasons=[reason])
            return {'safe': False, 'text': safe_text, 'blocked': True, 'reason': reason}

        # 4) Check candidate for hate/bias via classifier
        pred = self.hate_clf.predict([candidate])[0]
        if int(pred) == 1:
            blocked = True
            reason = 'Hate/bias content detected'
            safe_text = "I'm here to help, but I can't provide or repeat hateful or abusive content. If you need information on this topic, I can offer neutral, factual context."
            self.logger.log(user_input, candidate, blocked, reasons=[reason])
            return {'safe': False, 'text': safe_text, 'blocked': True, 'reason': reason}

        # 5) Passed all checks — safe to return
        blocked = False
        self.logger.log(user_input, candidate, blocked, reasons=[])
        return {'safe': True, 'text': candidate, 'blocked': False}


if __name__ == '__main__':
    bot = SafeChatbot()
    while True:
        u = input('You: ')
        out = bot.filter_and_respond(u)
        print('Bot:', out['text'])
